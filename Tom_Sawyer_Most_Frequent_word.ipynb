{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent word used in Tom Sawyer is tom.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "What are the most frequent words in Tom Sawyer, by Mark Twain novel, and how often do they occur?\n",
    "\n",
    "In this notebook, we'll scrape the novel Tom Sawyer from the website Project Gutenberg (which contains a large corpus of books) using the Python package requests. \n",
    "Then we'll extract words from this web data using BeautifulSoup. Finally, we'll dive into analyzing the distribution of words using the Natural Language ToolKit (nltk) and Counter.\n",
    "\n",
    "The Data Science pipeline we'll build in this notebook can be used to visualize the word frequency distributions of any novel that you can find on Project Gutenberg. \n",
    "The natural language processing tools used here apply to much of the data that data scientists encounter as a vast proportion of the world's data is unstructured data and includes a great deal of text.\n",
    "\n",
    "Let's start by loading in the three main Python packages we are going to use.\n",
    "\n",
    "'''\n",
    "\n",
    "# Importing requests, BeautifulSoup, nltk, and Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Getting the Tom Sawyer HTML \n",
    "r = requests.get('https://www.gutenberg.org/files/74/74-h/74-h.htm')\n",
    "\n",
    "# Setting the correct text encoding of the HTML page\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "# Creating a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Getting the text out of the soup\n",
    "text = soup.get_text()\n",
    "\n",
    "#nltk â€“ the Natural Language Toolkit\n",
    "# Creating a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Create a list called words containing all tokens transformed to lower-case\n",
    "words = []\n",
    "for w in tokens:\n",
    "    words.append(w.lower())\n",
    "\n",
    "# Getting the English stop words from nltk\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Create a list words_ns containing all words that are in words but not in sw\n",
    "\n",
    "words_ns = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in sw:\n",
    "        words_ns.append(w)\n",
    "\n",
    "# Initialize a Counter object from our processed list of words\n",
    "count = Counter(words_ns)\n",
    "\n",
    "# Store 10 most common words and their counts as top_ten\n",
    "top_twenty = count.most_common(20)\n",
    "\n",
    "print(f'The most frequent word used in Tom Sawyer is {top_twenty[0][0]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
